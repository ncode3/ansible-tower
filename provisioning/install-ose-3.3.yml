---
- include: firstboot.yml
- name: Launches new OSE 3.5 instance with provisioning userdata using Cloud-Init
  hosts: ose-cluster-master.dev.maskedadmins.com
  vars:
    multinode: "no"
    node1_ip: ""
    node1_hostname: ""
    node2_ip: ""
    node2_hostname: ""
    ose_add_user: "admin"
    ose_add_pass: "redhat123"
    ose_add_project: "redhat-lab"
    ose_add_project_description: "This is an example project to demonstrate OpenShift v3"
    ose_add_project_displayname: "Redhat Lab Demo Project"
    ose_outbound_network_cidr: "10.55.0.0/16"
  tasks:
    - name: "installing private key for root"
      template:
        src: openstack/blank.key.j2
        dest: /root/.ssh/id_rsa
        owner: root
        group: root
        mode: 0600

    - name: "installing ssh config for root"
      template:
        src: vms/ssh-config.j2
        dest: /root/.ssh/config
        owner: root
        group: root
        mode: 0600

    - name: "installing private key for cloud-user"
      template:
        src: openstack/blank.key.j2
        dest: /home/cloud-user/.ssh/id_rsa
        owner: cloud-user
        group: cloud-user
        mode: 0600

    - name: "installing ssh config for cloud-user"
      template:
        src: vms/ssh-config.j2
        dest: /home/cloud-user/.ssh/config
        owner: cloud-user
        group: cloud-user
        mode: 0600

    - name: "Enabling OSE needed repos"
      shell: |
        subscription-manager repos --enable={{ item }}
      with_items: "{{ ose_repos }}"
      become: true

    - name: "Enabling OSE needed repos"
      shell: |
        ssh root@{{ node1_ip }} 'subscription-manager repos --enable={{ item }}'
        ssh root@{{ node2_ip }} 'subscription-manager repos --enable={{ item }}'
      with_items: "{{ ose_repos }}"

    - name: "Install OSE yum packages"
      yum:
        name: "{{ item }}"
        state: latest
      with_items: "{{ ose_pkgs }}"
      become: true

    - file:
        path: /root/.config/openshift
        state: directory
        owner: root
        group: root
        mode: 0755
      become: true

    - file:
        path: /opt/ose-mount-host
        state: directory
        owner: 1001
        group: root
        mode: 0755
      become: true

    - template:
        src: ose/installer.cfg.yml.j2
        dest: /root/.config/openshift/installer.cfg.yml
        owner: root
        group: root
        mode: 0644
      when: multinode == "no"
      become: true

    - template:
        src: ose/hosts.j2
        dest: /root/.config/openshift/hosts
        owner: root
        group: root
        mode: 0644
      when: multinode == "no"
      become: true

    - template:
        src: ose/installer-multi.cfg.yml.j2
        dest: /root/.config/openshift/installer.cfg.yml
        owner: root
        group: root
        mode: 0644
      when: multinode == "yes"
      become: true

    - template:
        src: ose/hosts-multi.j2
        dest: /root/.config/openshift/hosts
        owner: root
        group: root
        mode: 0644
      when: multinode == "yes"
      become: true

    - template:
        src: ose/outbound-traffic.json.j2
        dest: /tmp/outbound-traffic.json
        owner: root
        group: root
        mode: 0644
      become: true

    - template:
        src: ose/docker-storage-setup.j2
        dest: /etc/sysconfig/docker-storage-setup
        owner: root
        group: root
        mode: 0644
      become: true

    - template:
        src: ose/docker.j2
        dest: /etc/sysconfig/docker
        owner: root
        group: root
        mode: 0644
      become: true

    - template:
        src: ose/fix-ose-auth.txt.j2
        dest: /tmp/fix-ose-auth.txt
        owner: root
        group: root
        mode: 0644
      become: true

    - template:
        src: ose/selinux.j2
        dest: /etc/sysconfig/selinux
        owner: root
        group: root
        mode: 0644
      become: true

    ## this is used by openshift dnsmasq to update the applications dns entry
    - template:
        src: ose/dns-updater.sh.j2
        dest: /opt/dns-updater.sh
        owner: root
        group: root
        mode: 0755
      become: true

    ## this is needed to update dns with the real hostname given in openstack
    - template:
        src: vms/dns-updater.sh.j2
        dest: /usr/sbin/ifup-local
        owner: root
        group: root
        mode: 0755
      become: true

    ## this is needed because a search domain will break things, so we update the dhcp script to comment it out
    - template:
        src: ose/99-origin-dns.sh.j2
        dest: /usr/share/ansible/openshift-ansible/roles/openshift_node_dnsmasq/files/networkmanager/99-origin-dns.sh
        owner: root
        group: root
        mode: 0755
      become: true

    - name: "Setup docker storage, requires a separate volume on /dev/vdb"
      shell: |
        docker-storage-setup
      become: true
      ignore_errors: yes
      args:
        creates: /root/.setupcomplete

    - name: "Enabling and restarting services {{ ose_svcs }}"
      service:
        name: "{{ item }}"
        state: restarted
        enabled: yes
      with_items: "{{ ose_svcs }}"
      become: true

#    - name: Wait for server to restart
#      local_action:
#        module: wait_for
#          host={{ ansible_host }}
#          port=22
#          delay=1
#          timeout=300
#
    - name: "Install OSE 3.5 - takes 50 mins for 3 nodes"
      shell: |
        setenforce 0
        atomic-openshift-installer -u install && touch /root/.setupcomplete
        lead='^    method: auto$'
        tail='^  masterCA: ca-bundle.crt$'
        sed -i -e "/$lead/,/$tail/{ /$lead/{p; r /tmp/fix-ose-auth.txt
        }; /$tail/p; d }"  /etc/origin/master/master-config.yaml
        htpasswd -c -b /etc/origin/master/htpasswd {{ ose_add_user }} {{ ose_add_pass }}
        ## create a new project
        oc login -u system:admin -n default
        oc new-project {{ ose_add_project }} \
        --description="{{ ose_add_project_description }}" \
        --display-name="{{ ose_add_project_displayname }}"
        oadm policy add-cluster-role-to-user cluster-admin {{ ose_add_user }}
        oadm policy add-role-to-user system:image-builder {{ ose_add_user }}
        oadm policy add-role-to-user admin {{ ose_add_user }} -n openshift
        sed -i -e 's/^#dhcp-script=\/bin\/echo/dhcp-script=\/opt\/dns-updater.sh/g' /etc/dnsmasq.conf
        grep dns-updater /usr/lib/systemd/system/dnsmasq.service || \
        sed -i -e 's/^ExecStart=\/usr\/sbin\/dnsmasq -k/ExecStart=\/usr\/sbin\/dnsmasq -k --dhcp-script=\/opt\/dns-updater.sh/g' \
        /usr/lib/systemd/system/dnsmasq.service
        systemctl daemon-reload
        systemctl restart dnsmasq
        systemctl restart atomic-openshift-master
        /opt/dns-updater.sh add mac {{ ansible_default_ipv4['address'] }} $(hostname --short)
        # create outbound policy rule
        #oc login -u {{ ose_add_user }} -p {{ ose_add_pass }}
        oc project {{ ose_add_project }}
        oc create -f /tmp/outbound-traffic.json
        # creating workshopper app
        oc new-app samueltauil/workshopper -e CONTENT_URL_PREFIX="https://raw.githubusercontent.com/samueltauil/openshiftv3-workshop/master" -e WORKSHOPS_URLS="https://raw.githubusercontent.com/samueltauil/openshiftv3-workshop/master/_module_groups.yml"
        oc expose service/workshopper
        /opt/dns-updater.sh add mac {{ ansible_default_ipv4['address'] }} workshopper-{{ ose_add_project }}
        oc new-app https://github.com/syspimp/simplephp
        oc expose service/simplephp
        /opt/dns-updater.sh add mac {{ ansible_default_ipv4['address'] }} simplephp-{{ ose_add_project }}
        echo "calling update firewall port for workshopper"
        curl -f -k -H 'Content-Type: application/json' -XPOST -d '{"extra_vars":"{\"int_ip\":\"{{ ansible_default_ipv4['address'] }}\",\"int_port\":\"80\",\"ext_port\":\"8444\"}"}' --user admin:ansible https://10.55.102.248:443/api/v1/job_templates/660/launch/
        # this command will make the master a scheduled node. since we are launching multinode, lets use those and comment this out
        echo -e '#!/bin/bash\noadm manage-node ose-cluster-master.dev.maskedadmins.com --schedulable\noc project default\noc deploy router --retry\noc deploy registry-console --retry' > /root/fix-demo.sh
        chmod +x /root/fix-demo.sh
        /root/fix-demo.sh
        sleep 120
        echo "calling app deploy because I am lazy"
        curl -f -k -H 'Content-Type: application/json' -XPOST --user admin:ansible https://10.55.102.248:443/api/v1/job_templates/239/launch/
        echo "calling update firewall port for ose"
        curl -f -k -H 'Content-Type: application/json' -XPOST -d '{"extra_vars":"{\"int_ip\":\"{{ ansible_default_ipv4['address'] }}\",\"int_port\":\"8443\",\"ext_port\":\"8443\"}"}' --user admin:ansible https://10.55.102.248:443/api/v1/job_templates/660/launch/
      args:
        creates: /root/.setupcomplete
      become: true
      ignore_errors: yes

- include: cfme-add-openshift.yml
